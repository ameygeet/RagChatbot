
# ğŸ“š RAG-Based Local Document Chatbot

A **Streamlit web app** that allows you to upload your own documents and chat with them using **Retrieval-Augmented Generation (RAG)** â€” all powered by **local LLMs** via [Ollama](https://ollama.com). No internet or external API keys required!

---

## ğŸš€ Features

- ğŸ“„ Upload `.pdf`, `.txt`, `.docx`, or `.md` files
- ğŸ” Uses **LangChain** and **FAISS** to chunk and embed documents
- ğŸ§  Powered by **Gemma:2b** via Ollama for question-answering
- ğŸ“‘ Auto-generates a summary of your uploaded document
- ğŸ’¬ Ask natural language questions about the content
- ğŸ–¥ï¸ Fully local â€“ no external API calls

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit
- **Backend**: LangChain, FAISS, Ollama LLM
- **LLM Model**: Gemma:2b (via Ollama)
- **Embeddings**: nomic-embed-text
- **Document Support**: PDF, TXT, DOCX, MD

---

## ğŸ“‚ Project Structure

```
ğŸ“ your_project/
â”œâ”€â”€ app.py               # Main Streamlit UI
â”œâ”€â”€ loader.py            # Handles document loading and chunking
â”œâ”€â”€ rag_chain.py         # Builds vector store and QA/summarization chain
â”œâ”€â”€ docs/                # Uploaded documents
â””â”€â”€ README.md            # Project README
```

---

## ğŸ“¦ Installation

1. **Install Python dependencies**:
   ```bash
   pip install streamlit langchain faiss-cpu unstructured
   ```

2. **Install and run Ollama**:  
   Follow [https://ollama.com](https://ollama.com) to install Ollama, then run:
   ```bash
   ollama pull gemma:2b
   ```

3. **Run the Streamlit App**:
   ```bash
   streamlit run app.py
   ```

---

## ğŸ“¸ Demo

![screenshot](https://via.placeholder.com/800x400.png?text=Upload+your+doc+and+chat+with+it)

---

## ğŸ”— GitHub Project

Feel free to check out and contribute:  
**[ğŸ‘‰ GitHub Repo](https://github.com/your-username/rag-chatbot)**

---

## ğŸ“ƒ License

This project is licensed under the MIT License.
